
@thesis{scheithauer_reconnaissanc_2021,
	title = {La reconnaissanc d'entités nommées appliquées à des données issues de la transcription automatique de documents manuscrits patrimoniaux. Expérimentations et préconisations à partir du projet {LECTAUREP}},
	url = {https://raw.githubusercontent.com/HugoSchtr/memoire_TNAH_M2_HugoScheithauer/main/memoire_Hugo_Scheithauer_TNAH.pdf},
	institution = {Ecole nationale des chartes},
	type = {Mémoire de master "Technologies numériques appliquées à l'histoire"},
	author = {Scheithauer, Hugo},
	urldate = {2022-05-29},
	date = {2021},
}

@article{stutzmann_recherche_2017,
	title = {La recherche en plein texte dans les sources manuscrites médiévales : enjeux et perspectives du projet {HIMANIS} pour l’édition électronique},
	volume = {73},
	rights = {Tous droits réservés},
	issn = {0751-2708},
	url = {https://journals.openedition.org/medievales/8198},
	doi = {10.4000/medievales.8198},
	shorttitle = {La recherche en plein texte dans les sources manuscrites médiévales},
	abstract = {{HIMANIS} (Historical {MANuscript} Indexing for user-controlled Search) est un projet de recherche européen, associant, sous le pilotage de l’{IRHT} ({CNRS}, France), la société innovante A2iA (France), la Rijksuniversiteit Groningen (Pays-Bas) et l’Universitat Politècnica de València (Espagne). Il vise à l’indexation du texte des registres de la chancellerie royale française des années 1302-1483, conservés aux Archives nationales sous les cotes {JJ}35 à {JJ}211, à partir des images produites par leur numérisation. Les enjeux de recherche d’information (données massives et bruitées) permettent de conjoindre les enjeux technologiques (reconnaissance de l’écriture manuscrite) et historiques (analyses paléographiques et diplomatiques, recherche sur les institutions, le fonctionnement de la monarchie, la naissance de l’État-nation). La présente contribution propose un modèle d’accès à l’information dans un corpus de données massives, d’un point de vue tant ergonomique qu’herméneutique. À cette fin, après une présentation du corpus, des outils actuels pour accéder à l’information qu’ils contiennent et de leur formalisation en {TEI}, elle problématise l’édition électronique comme « vérité terrain » et « terrain d’apprentissage », en renversant l’approche classique de l’édition critique comme finalité. Enfin, elle décrit le modèle d’accès proposé, à la fois pour une approche par « indexation » (et non par transcription) et pour une granularité par acte.},
	pages = {67--96},
	number = {73},
	journaltitle = {Médiévales. Langues, Textes, Histoire},
	author = {Stutzmann, Dominique and Moufflet, Jean-François and Hamel, Sébastien},
	urldate = {2022-04-20},
	date = {2017-12-15},
	langid = {french},
	keywords = {Archives nationales, chancellerie royale, édition électronique, {HIMANIS}, humanités numériques, recherche en plein texte},
}

@thesis{janes_du_2021,
	location = {Paris},
	title = {Du catalogue papier au numérique Une chaîne de traitement ouverte pour l’extraction d’informations issues de documents structurés},
	url = {https://raw.githubusercontent.com/Juliettejns/Memoire_TNAH/main/Jjanes_Memoire.pdf},
	institution = {Ecole nationale des chartes},
	type = {Mémoire de master "Technologies numériques appliquées à l'histoire"},
	author = {Janès, Juliette},
	urldate = {2022-05-29},
	date = {2021},
}

@thesis{ehrmann_les_2008,
	title = {Les entités nommées, de la linguistique au {TAL} : statut théorique et méthodes de désambiguïsation},
	rights = {Licence Etalab},
	url = {https://hal.archives-ouvertes.fr/tel-01639190/document},
	shorttitle = {Les entités nommées, de la linguistique au {TAL}},
	abstract = {Le traitement des entités nommées fait aujourd'hui figure d'incontournable en Traitement Automatique des Langues. Apparue au milieu des années 1990, la tâche de reconnaissance et de catégorisation des noms de personnes, de lieux, d'organisations, etc. Apparaît en effet comme fondamentale pour diverses applications participant de l'analyse de contenu et nombreux sont les travaux se consacrant à sa mise en oeuvre, obtenant des résultats plus qu'honorables. Fort de ce succès, le traitement des entités nommées s'oriente désormais vers de nouvelles perspectives, avec la désambiguïsation et une annotation enrichie de ces unités. Ces nouveaux défis rendent cependant d'autant plus cruciale la question du statut théorique des entités nommées, lequel n'a guère été discuté jusqu'à aujourd'hui. Deux axes de recherche ont été investis durant ce travail de thèse avec, d'une part, la proposition d'une définition des entités nommées et, d'autre part, des méthodes de désambiguïsation. A la suite d'un état des lieux de la tâche de reconnaissance de ces unités, il fut nécessaire d'examiner, d'un point de vue méthodologique, comment aborder la question de la définition les entités nommées. La démarche adoptée invita à se tourner du côté de la linguistique (noms propres et descriptions définies) puis du côté du traitement automatique, ce parcours visant au final à proposer une définition tenant compte tant des aspects du langage que des exigences des systèmes informatiques. La suite du mémoire rend compte d'un travail davantage expérimental, avec l'exposé d'une méthode d'annotation fine tout d'abord, de résolution de métonymie enfin.},
	institution = {Paris 7},
	type = {These de doctorat},
	author = {Ehrmann, Maud},
	editora = {Victorri, Bernard},
	editoratype = {collaborator},
	urldate = {2022-04-14},
	date = {2008-01-01},
	keywords = {Traitement automatique du langage naturel, Catégorisation (linguistique), Linguistique, Métonymie, Référence (linguistique)},
}

@thesis{stern_identification_2013,
	title = {Identification automatique d'entités pour l'enrichissement de contenus textuels},
	url = {https://tel.archives-ouvertes.fr/tel-00939420},
	abstract = {Cette thèse propose une méthode et un système d'identification d'entités (personnes, lieux, organisations) mentionnées au sein des contenus textuels produits par l'Agence France Presse dans la perspective de l'enrichissement automatique de ces contenus. Les différents domaines concernés par cette tâche ainsi que par l'objectif poursuivi par les acteurs de la publication numérique de contenus textuels sont abordés et mis en relation : Web Sémantique, Extraction d'Information et en particulier Reconnaissance d'Entités Nommées ({\textbackslash}ren), Annotation Sémantique, Liage d'Entités. À l'issue de cette étude, le besoin industriel formulé par l'Agence France Presse fait l'objet des spécifications utiles au développement d'une réponse reposant sur des outils de Traitement Automatique du Langage. L'approche adoptée pour l'identification des entités visées est ensuite décrite : nous proposons la conception d'un système prenant en charge l'étape de {\textbackslash}ren à l'aide de n'importe quel module existant, dont les résultats, éventuellement combinés à ceux d'autres modules, sont évalués par un module de Liage capable à la fois (i) d'aligner une mention donnée sur l'entité qu'elle dénote parmi un inventaire constitué au préalable, (ii) de repérer une dénotation ne présentant pas d'alignement dans cet inventaire et (iii) de remettre en cause la lecture dénotationnelle d'une mention (repérage des faux positifs). Le système {\textbackslash}nomos est développé à cette fin pour le traitement de données en français. Sa conception donne également lieu à la construction et à l'utilisation de ressources ancrées dans le réseau des {\textbackslash}ld ainsi que d'une base de connaissances riche sur les entités concernées.},
	institution = {Université Paris-Diderot - Paris {VII}},
	type = {Thèse de doctorat},
	author = {Stern, Rosa},
	urldate = {2022-03-28},
	date = {2013},
	langid = {french},
	file = {Full Text PDF:C\:\\Users\\virgi\\Zotero\\storage\\GFFYDQED\\Stern - 2013 - Identification automatique d'entités pour l'enrich.pdf:application/pdf;Snapshot:C\:\\Users\\virgi\\Zotero\\storage\\PRN375SP\\tel-00939420.html:text/html;Snapshot:C\:\\Users\\virgi\\Zotero\\storage\\QMQI4VIM\\tel-00939420.html:text/html},
}

@article{holtz_les_2000,
	title = {Les premières années de l’Institut de recherche et d’histoire des textes},
	rights = {All rights reserved},
	issn = {1298-9800},
	url = {https://journals.openedition.org/histoire-cnrs/2742?&id=2742},
	doi = {10.4000/histoire-cnrs.2742},
	abstract = {The First Years of The Institute for the Research of Texts and their History The idea for the founding of the Institute for the Research of Texts and their History ({IRHT}) originated with the historian, Félix Grat, archivist paleographer and former member of the Ecole Francaise in Rome, later elected to the National Assembly. As early as 1937, two years before the creation of {CNRS}, he succeeded in convincing Jean Perrin, Nobel Prize winner in physics and then Under Secretary of State for Science and Research under Prime Minister Léon Blum, of the importance of a project whose goal was nothing less than to assure the conservation of the written memory of human thought. This resulted in the foundation of the first laboratory for research in a domain other than the exact sciences. To be sure, for this classical scholar, the first priority to be carried out was the transmission of those works that had first seen the light of day in manuscript form, in particular the great writers of Ancient Rome. Right from the beginning however F. Grat laid out an ambitious program with his plan for an Arabic section (a goal that he took particularly to heart for reasons that were as scientific as they were political), as well as Greek, French, Celtic sections and so on. Being himself particularly keen on all the progress accomplished in photography, F. Grat wanted a huge library that would collect photographs of all written manuscripts spread throughout the world in diverse languages in order to make them accessible to researchers and to facilitate research. With the help of Jeanne Vieillard, who was first place in the class of 1924 at the Ecole des chartes, Félix Grat opened the new institute located at first in the Bibliothèque Nationale. He sent out his assistants throughout Europe to photograph the manuscripts. However, war was threatening and once it broke out, the patriotic F. Grat enlisted in an auxiliary corps, while the {IRHT} withdrew to Laval He was one of the first officers to fail at the head of his troops at the very beginning of the German offensive. Jeanne Vieillard took over the direction of the {IRHT}. Specialized sections were founded one after another, going even beyond the limits of the program planned by Félix Grat. At the end of 1940, the {IRHT} relocated to the National Archives and, in 1960, was transferred to a building constructed by {CNRS} on the Quai Anatole-France in Paris. Then followed twenty years of accumulating first rate documentation on each author, each text, each manuscript and in all the disciplines bordering on textual history. This was an institute ahead of its time due to its organization, specialization, technical nature and feeling for multidisciplinary research. {IRHT} was admired by users from all countries. The staff, well supervised by the archivists paleographers, was constantly growing and so were the programs. Already collections were established that would consolidate the international status of this laboratory which under Jean Glénisson, successor to Jeanne Vielliard, was to enjoy renewed momentum.},
	number = {2},
	journaltitle = {La revue pour l’histoire du {CNRS}},
	author = {Holtz, Louis},
	urldate = {2022-07-16},
	date = {2000-05-05},
	langid = {french},
	note = {{ISBN}: 9782271057082
Number: 2
Publisher: {CNRS} Éditions},
	keywords = {{IRHT}},
	file = {Full Text PDF:C\:\\Users\\virgi\\Zotero\\storage\\ZX8NNULE\\Holtz - 2000 - Les premières années de l’Institut de recherche et.pdf:application/pdf},
}

@online{stutzmann_compte-rendu_nodate,
	title = {Compte-rendu final du projet {ORIFLAMMS} / {ORIFLAMMS} Final report},
	url = {https://oriflamms.hypotheses.org/1592},
	abstract = {Lien vers le rapport complet / Link to the full report: D. Stutzmann, Projet {ANR}-12-{CORP}-0010 Oriflamms: Compte-rendu de fin de projet, octobre 2016, 31 p. [en ligne] http://oriflamms.hypotheses.org/files/2017/04/Oriflamms-Compte-rendu-final.pdf English version below   L’écriture du Moyen Âge : un objet sous le regard croisé des Humanités et des Sciences de l’ingénieur  Comprendre les écritures dans une approche … Continuer la lecture de Compte-rendu final du projet {ORIFLAMMS} / {ORIFLAMMS} Final report  →},
	titleaddon = {Écriture médiévale \& numérique},
	type = {Billet},
	author = {Stutzmann, Dominique},
	urldate = {2022-07-16},
	langid = {french},
	file = {Snapshot:C\:\\Users\\virgi\\Zotero\\storage\\GIAE2CFD\\1592.html:text/html},
}

@thesis{potin_mise_2007,
	location = {Paris},
	title = {La mise en archives du trésor des chartes ({XIIIe}-{XIXe} siècle)},
	url = {http://theses.enc.sorbonne.fr/2007/potin},
	institution = {Ecole nationale des chartes},
	type = {Positions de thèse pour le diplôme d'archiviste-paléographe},
	author = {Potin, Yann},
	urldate = {2022-07-16},
	date = {2007},
	file = {theses.enc.sorbonne.fr/2007/potin:C\:\\Users\\virgi\\Zotero\\storage\\JJGI3XSD\\potin.html:text/html},
}

@online{noauthor_himanis_nodate,
	title = {Himanis - Chancery Indexing and Search},
	url = {http://himanis.huma-num.fr/app//},
	urldate = {2022-07-16},
	file = {Himanis - Chancery Indexing and Search:C\:\\Users\\virgi\\Zotero\\storage\\NQNXJU6J\\himanis.huma-num.fr.html:text/html},
}

@online{noauthor_bvmm_nodate,
	title = {{BVMM}},
	url = {https://bvmm.irht.cnrs.fr/},
	urldate = {2022-07-16},
	file = {BVMM:C\:\\Users\\virgi\\Zotero\\storage\\N53X2X7F\\bvmm.irht.cnrs.fr.html:text/html},
}

@inproceedings{bluche_automatic_2016,
	location = {Santorini, France},
	title = {Automatic Handwritten Character Segmentation for Paleographical Character Shape Analysis},
	url = {https://www.researchgate.net/profile/Christopher-Kermorvant/publication/303950834_Automatic_Handwritten_Character_Segmentation_for_Paleographical_Character_Shape_Analysis/links/5a1bd9d54585155c26ae0850/Automatic-Handwritten-Character-Segmentation-for-Paleographical-Character-Shape-Analysis.pdf?origin=publication_detail},
	doi = {10.1109/DAS.2016.74},
	series = {2016 12th {IAPR} Workshop on Document Analysis Systems ({DAS})},
	abstract = {Written texts are both abstract and physical objects: ideas, signs and shapes, whose meanings, graphical systems and social connotations evolve through time. To study this dual nature of texts, paleographers need to analyse large scale corpora at the finest granularity, such as character shape. This goal can only be reached through an automatic segmentation process. In this paper, we present a method, based on Handwritten Text Recognition, to automatically align images of digitized manuscripts with texts from scholarly editions, at the levels of page, column, line, word, and character. It has been successfully
applied to two datasets of medieval manuscripts, which are now almost fully segmented at character level. The quality of the word and character segmentations are evaluated and further paleographical analysis are presented.},
	pages = {42--47},
	booktitle = {2016 12th {IAPR} Workshop on Document Analysis Systems ({DAS})},
	publisher = {{IEEE}},
	author = {Bluche, Théodore and Stutzmann, Dominique and Kermorvant, Christopher},
	urldate = {2022-07-16},
	date = {2016-04},
	keywords = {Text recognition, automatic text recognition, Character recognition, Error analysis, Hidden Markov models, Image segmentation, Paleography, Shape, Training, word and character segmentation},
	file = {HAL Snapshot:C\:\\Users\\virgi\\Zotero\\storage\\GGBD8YH2\\hal-02425715.html:text/html},
}

@inproceedings{bluche_preparatory_2017,
	title = {Preparatory {KWS} Experiments for Large-Scale Indexing of a Vast Medieval Manuscript Collection in the {HIMANIS} Project},
	volume = {01},
	url = {http://www.jpuigcerver.net/pubs/bluche_icdar2017.pdf},
	doi = {10.1109/ICDAR.2017.59},
	abstract = {Making large-scale collections of digitized historical documents searchable is being earnestly demanded by many archives and libraries. Probabilistically indexing the text images of these collections by means of keyword spotting techniques is currently seen as perhaps the only feasible approach to meet this demand. A vast medieval manuscript collection, written in both Latin and French, called "Chancery", is currently being considered for indexing at large. In addition to its bilingual nature, one of the major difficulties of this collection is the very high rate of abbreviated words which, on the other hand, are completely expanded in the ground truth transcripts available. In preparation to undertake full indexing of Chancery, experiments have been carried out on a relatively small but fully representative subset of this collection. To this end, a keyword spotting approach has been adopted which computes word relevance probabilities using character lattices produced by a recurrent neural network and a N-gram character language model. Results confirm the viability of the chosen approach for the large-scale indexing aimed at and show the ability of the proposed modeling and training approaches to properly deal with the abbreviation difficulties mentioned.},
	eventtitle = {2017 14th {IAPR} International Conference on Document Analysis and Recognition ({ICDAR})},
	pages = {311--316},
	booktitle = {2017 14th {IAPR} International Conference on Document Analysis and Recognition ({ICDAR})},
	author = {Bluche, Théodore and Hamel, Sebastien and Kermorvant, Christopher and Puigcerver, Joan and Stutzmann, Dominique and Toselli, Alejandro H. and Vidal, Enrique},
	urldate = {2022-07-11},
	date = {2017-11},
	note = {{ISSN}: 2379-2140},
	keywords = {Hidden Markov models, character lattice, Decoding, Electronic mail, Indexing, indexing historical manuscript, keyword spotting, Lattices, Probabilistic logic, recurrent neural network, Recurrent neural networks},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\virgi\\Zotero\\storage\\RQXXECTB\\8269990.html:text/html},
}

@inproceedings{torres_aguilar_named_2021,
	location = {Helsinki, Finland},
	title = {Named Entity Recognition for French medieval charters},
	url = {https://hal.archives-ouvertes.fr/hal-03503055},
	series = {Workshop on Natural Language Processing for Digital Humanities Proceedings of the Workshop},
	abstract = {This paper presents the process of annotating and modelling a corpus to automatically detect named entities in medieval charters in French. It introduces a new annotated corpus and a new system which outperforms state-of-the art libraries. Charters are legal documents and among the most important historical sources for medieval studies as they reflect economic and social dynamics as well as the evolution of literacy and writing practices. Automatic detection of named entities greatly improves the access to these unstructured texts and facilitates historical research. The experiments described here are based on a corpus encompassing about 500k words (1200 charters) coming from three charter collections of
the 13th and 14th centuries. We annotated the corpus and then trained two state-of-the art {NLP} libraries for Named Entity Recognition (Spacy and Flair) and a custom neural model (Bi-{LSTM}-{CRF}). The evaluation shows that all three models achieve a high performance rate on the test set and a high generalization capacity against two external corpora unseen during training. This paper describes the corpus and the annotation model, and discusses the issues related to the linguistic processing of medieval French and formulaic discourse, so as to interpret the results within a larger historical perspective.},
	booktitle = {Workshop on Natural Language Processing for Digital Humanities},
	author = {Torres Aguilar, Sergio and Stutzmann, Dominique},
	urldate = {2022-07-17},
	date = {2021-12},
	keywords = {reconnaissance des entités nommées, ancien et moyen français, cultural heritage, named entity recognition, natural language processing, Old and Middle French, patrimoine culturel, traitement automatique du langage naturel},
	file = {HAL PDF Full Text:C\:\\Users\\virgi\\Zotero\\storage\\6CVSH42U\\Torres Aguilar et Stutzmann - 2021 - Named Entity Recognition for French medieval chart.pdf:application/pdf},
}

@inproceedings{boros_comparison_2020,
	title = {A comparison of sequential and combined approaches for named entity recognition in a corpus of handwritten medieval charters},
	url = {https://teklia.com/publications/ICFHR2020_NER_Comparison_final_updated.pdf},
	doi = {10.1109/ICFHR2020.2020.00025},
	abstract = {This paper introduces a new corpus of multilingual medieval handwritten charter images, annotated with full transcription and named entities. The corpus is used to compare two approaches for named entity recognition in historical document images in several languages: on the one hand, a sequential approach, more commonly used, that sequentially applies handwritten text recognition ({HTR}) and named entity recognition ({NER}), on the other hand, a combined approach that simultaneously transcribes the image text line and extracts the entities. Experiments conducted on the charter corpus in Latin, early new high German and old Czech for name, date and location recognition demonstrate a superior performance of the combined approach.},
	eventtitle = {2020 17th International Conference on Frontiers in Handwriting Recognition ({ICFHR})},
	pages = {79--84},
	booktitle = {2020 17th International Conference on Frontiers in Handwriting Recognition ({ICFHR})},
	author = {Boroş, Emanuela and Romero, Verónica and Maarand, Martin and Zenklová, Kateřina and Křečková, Jitka and Vidal, Enrique and Stutzmann, Dominique and Kermorvant, Christopher},
	date = {2020-09},
	keywords = {Text recognition, Adaptive optics, Handwriting recognition, Handwritten Text Recognition, historical document processing, Mathematical model, multilingualism, Named entity recognition, Neural networks, Optical character recognition software, Optical imaging},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\virgi\\Zotero\\storage\\4CQINEHL\\9257761.html:text/html},
}

@article{chastang_named_nodate,
	title = {A Named Entity Recognition Model for Medieval Latin Charters},
	volume = {015},
	issn = {1938-4122},
	number = {4},
	journaltitle = {Digital Humanities Quarterly},
	shortjournal = {{DHQ}},
	author = {Chastang, Pierre and Aguilar, Sergio Torres and Tannier, Xavier},
	file = {DHQ\: Digital Humanities Quarterly\: A Named Entity Recognition Model for Medieval Latin Charters:C\:\\Users\\virgi\\Zotero\\storage\\J5ZXFLTV\\000574.html:text/html},
}

@misc{stutzmann_home-alcar_2021,
	title = {{HOME}-Alcar: Aligned and Annotated Cartularies},
	url = {https://zenodo.org/record/5600884},
	shorttitle = {{HOME}-Alcar},
	abstract = {The {HOME}-Alcar (Aligned and Annotated Cartularies) corpus was produced as part of the European research project {HOME} History of Medieval Europe (https://www.heritageresearch-hub.eu/project/home/), led under the coordination oflinebreakof Institut de Recherche et d'Histoire des Textes ({PI}: D. Stutzmann), with the Universitat Politecnica de Valencia ({PI}: E. Vidal), the National Archives of the Czech Republic in Prague ({PI}: J. Kreckova), and Teklia {SAS} ({PI}: C. Kermorvant) The {HOME}-Alcar (Aligned and Annotated Cartularies) corpus is a resource created to train Handwritten Text Recognition ({HTR}) and Named Entity Recognition ({NER}), and presents a collection of (i) digital images of 17 medieval manuscripts; (ii) scholarly editions thereof; (iii) coordinates linking images and text at line level; (iv) annotations of Named Entities (place and person names). The 17 medieval manuscripts in this corpus are cartularies, i.e. books copying charters and legal acts, produced between the 12th and 14th centuries.},
	publisher = {Zenodo},
	author = {Stutzmann, Dominique and Torres Aguilar, Sergio and Chaffenet, Paul},
	urldate = {2022-07-17},
	date = {2021-10-26},
	doi = {10.5281/zenodo.5600884},
	note = {Type: dataset},
	keywords = {named entity recognition, handwritten text recognition, Latin palaeography},
	file = {Zenodo Full Text PDF:C\:\\Users\\virgi\\Zotero\\storage\\KA9LWSAW\\Stutzmann et al. - 2021 - HOME-Alcar Aligned and Annotated Cartularies.pdf:application/pdf},
}

@book{poibeau_extraction_2003,
	title = {Extraction automatique d'information : Du texte brut au web sémantique},
	url = {https://hal.archives-ouvertes.fr/hal-00005506},
	shorttitle = {Extraction automatique d'information},
	abstract = {Les entreprises et les particuliers sont confrontés à une masse d'information sans cesse croissante. Partant de ce constat, de nombreux systèmes ont été conçus pour filtrer, trier et catégoriser l'information. L'offre est en revanche beaucoup plus faible en ce qui concerne l'analyse du contenu. Extraction automatique d'information - du texte brut au web sémantique présente les progrès récents en extraction d'information et en compréhension de textes. Les recherches effectuées ces dernières années dans le domaine du traitement automatique des langues rendent en effet possible l'annotation sémantique de documents, l'extraction d'information pertinente et la création de bases de connaissances structurées à partir de textes en langage naturel. L'ouvrage rappelle les grands courants de recherche qui ont marqué le domaine de la compréhension automatique de textes par ordinateur. Il se poursuit par la présentation détaillée d'un système appelé {SEMTEX}, qui est appliqué à une grande variété de textes et de situations différentes. Les applications détaillées donnent des perspectives sur le web sémantique et l'ingénierie des connaissances.},
	pagetotal = {238},
	publisher = {Lavoisier},
	author = {Poibeau, Thierry},
	urldate = {2022-07-18},
	date = {2003},
	keywords = {linguistique, extraction d'information, informatique, recherche d'information, traitement des langues},
}

@inproceedings{agirre_matching_2012,
	location = {Istanbul, Turkey},
	title = {Matching Cultural Heritage items to Wikipedia},
	url = {http://www.lrec-conf.org/proceedings/lrec2012/pdf/1021_Paper.pdf},
	abstract = {Digitised Cultural Heritage ({CH}) items usually have short descriptions and lack rich contextual information. Wikipedia articles, on the contrary, include in-depth descriptions and links to related articles, which motivate the enrichment of {CH} items with information from Wikipedia. In this paper we explore the feasibility of finding matching articles in Wikipedia for a given Cultural Heritage item. We manually annotated a random sample of items from Europeana, and performed a qualitative and quantitative study of the issues and problems that arise, showing that each kind of {CH} item is different and needs a nuanced definition of what “matching article” means. In addition, we test a well-known wikification (aka entity linking) algorithm on the task. Our results indicate that a substantial number of items can be effectively linked to their corresponding Wikipedia article.},
	eventtitle = {{LREC} 2012},
	pages = {1729--1735},
	booktitle = {Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12)},
	publisher = {European Language Resources Association ({ELRA})},
	author = {Agirre, Eneko and Barrena, Ander and de Lacalle, Oier Lopez and Soroa, Aitor and Fernando, Samuel and Stevenson, Mark},
	urldate = {2022-07-21},
	date = {2012-05},
	file = {Full Text PDF:C\:\\Users\\virgi\\Zotero\\storage\\EHY7TKBN\\Agirre et al. - 2012 - Matching Cultural Heritage items to Wikipedia.pdf:application/pdf},
}

@inproceedings{munnelly_investigating_2018,
	location = {New York, {NY}, {USA}},
	title = {Investigating Entity Linking in Early English Legal Documents},
	isbn = {978-1-4503-5178-2},
	url = {https://doi.org/10.1145/3197026.3197055},
	doi = {10.1145/3197026.3197055},
	series = {{JCDL} '18},
	abstract = {In this paper we investigate the accuracy and overall suitability of a variety of Entity Linking systems for the task of disambiguating entities in 17th century depositions obtained during the 1641 Irish Rebellion. The depositions are extremely difficult for modern {NLP} tools to work with due to inconsistent spelling, use of language and archaic references. In order to assess the severity of difficulty faced by Entity Linking systems when working with the depositions we use them to create an evaluation corpus. This corpus is used as an input to the General Entity Annotator Benchmarking Framework a standard benchmarking platform for entity annotation systems. Based on this corpus and the results obtained from General Entity Annotator Benchmarking Framework we observe that the accuracy of existing Entity Linking systems is lacking when applied to content like these depositions. This is due to a number of issues ranging from problems with existing state-of-the-art systems to poor representation of historic entities in modern knowledge bases. We discuss some interesting questions raised by this evaluation and put forward a plan for future work in order to learn more.},
	pages = {59--68},
	booktitle = {Proceedings of the 18th {ACM}/{IEEE} on Joint Conference on Digital Libraries},
	publisher = {Association for Computing Machinery},
	author = {Munnelly, Gary and Lawless, Seamus},
	urldate = {2022-07-21},
	date = {2018-05-23},
	keywords = {cultural heritage, digital humanities, named entity disambiguation},
	file = {Texte intégral:C\:\\Users\\virgi\\Zotero\\storage\\XEUC734I\\Munnelly et Lawless - 2018 - Investigating Entity Linking in Early English Lega.pdf:application/pdf},
}

@inproceedings{kolitsas_end--end_2018,
	location = {Brussels, Belgium},
	title = {End-to-End Neural Entity Linking},
	url = {https://aclanthology.org/K18-1050},
	doi = {10.18653/v1/K18-1050},
	abstract = {Entity Linking ({EL}) is an essential task for semantic text understanding and information extraction. Popular methods separately address the Mention Detection ({MD}) and Entity Disambiguation ({ED}) stages of {EL}, without leveraging their mutual dependency. We here propose the first neural end-to-end {EL} system that jointly discovers and links entities in a text document. The main idea is to consider all possible spans as potential mentions and learn contextual similarity scores over their entity candidates that are useful for both {MD} and {ED} decisions. Key components are context-aware mention embeddings, entity embeddings and a probabilistic mention - entity map, without demanding other engineered features. Empirically, we show that our end-to-end method significantly outperforms popular systems on the Gerbil platform when enough training data is available. Conversely, if testing datasets follow different annotation conventions compared to the training set (e.g. queries/ tweets vs news documents), our {ED} model coupled with a traditional {NER} system offers the best or second best {EL} accuracy.},
	eventtitle = {{CoNLL} 2018},
	pages = {519--529},
	booktitle = {Proceedings of the 22nd Conference on Computational Natural Language Learning},
	publisher = {Association for Computational Linguistics},
	author = {Kolitsas, Nikolaos and Ganea, Octavian-Eugen and Hofmann, Thomas},
	urldate = {2022-04-21},
	date = {2018},
	file = {Full Text PDF:C\:\\Users\\virgi\\Zotero\\storage\\GU4SQMBD\\Kolitsas et al. - 2018 - End-to-End Neural Entity Linking.pdf:application/pdf},
}

@inproceedings{hosseini_deezymatch_2020,
	title = {{DeezyMatch}: A Flexible Deep Learning Approach to Fuzzy String Matching},
	doi = {10.18653/v1/2020.emnlp-demos.9},
	shorttitle = {{DeezyMatch}},
	abstract = {We present {DeezyMatch}, a free, open-source software library written in Python for fuzzy string matching and candidate ranking. Its pair classifier supports various deep neural network architectures for training new classifiers and for fine-tuning a pretrained model, which paves the way for transfer learning in fuzzy string matching. This approach is especially useful where only limited training examples are available. The learned {DeezyMatch} models can be used to generate rich vector representations from string inputs. The candidate ranker component in {DeezyMatch} uses these vector representations to find, for a given query, the best matching candidates in a knowledge base. It uses an adaptive searching algorithm applicable to large knowledge bases and query sets. We describe {DeezyMatch}’s functionality, design and implementation, accompanied by a use case in toponym matching and candidate ranking in realistic noisy datasets.},
	author = {Hosseini, Kasra and Nanni, Federico and Coll Ardanuy, Mariona},
	date = {2020-10-01},
	file = {Full Text PDF:C\:\\Users\\virgi\\Zotero\\storage\\YQ42J8QD\\Hosseini et al. - 2020 - DeezyMatch A Flexible Deep Learning Approach to F.pdf:application/pdf},
}

@article{soudani_adapting_nodate,
	title = {Adapting a system for Named Entity Recognition and Linking for 19th century French Novels},
	pages = {2},
	author = {Soudani, Aicha},
	langid = {english},
	file = {Soudani - Adapting a system for Named Entity Recognition and.pdf:C\:\\Users\\virgi\\Zotero\\storage\\TETYM8A8\\Soudani - Adapting a system for Named Entity Recognition and.pdf:application/pdf},
}

@article{frontini_annotation_2016,
	title = {Annotation of Toponyms in {TEI} Digital Literary Editions and Linking to the Web of Data},
	url = {https://hal.archives-ouvertes.fr/hal-01363709},
	doi = {10.14195/2182-8830_4-2_3},
	number = {2},
	journaltitle = {{MALTIT} : Materialities of literature},
	author = {Frontini, Francesca and Brando, Carmen and Riguet, Marine and Jacquot, Clémence and Jolivet, Vincent},
	urldate = {2022-07-21},
	date = {2016-07},
	keywords = {digital literary studies, geographic databases, maps and visualizations, semantic web, toponyms},
	file = {HAL PDF Full Text:C\:\\Users\\virgi\\Zotero\\storage\\5C8PTHNF\\Frontini et al. - 2016 - Annotation of Toponyms in TEI Digital Literary Edi.pdf:application/pdf},
}

@inproceedings{suarez_establishing_2020,
	title = {Establishing a New State-of-the-Art for French Named Entity Recognition},
	url = {https://hal.inria.fr/hal-02617950},
	abstract = {The French {TreeBank} developed at the University Paris 7 is the main source of morphosyntactic and syntactic annotations for French. However, it does not include explicit information related to named entities, which are among the most useful information for several natural language processing tasks and applications. Moreover, no large-scale French corpus with named entity annotations contain referential information, which complement the type and the span of each mention with an indication of the entity it refers to. We have manually annotated the French {TreeBank} with such information, after an automatic pre-annotation step. We sketch the underlying annotation guidelines and we provide a few figures about the resulting annotations.},
	eventtitle = {{LREC} 2020 - 12th Language Resources and Evaluation Conference},
	author = {Suárez, Pedro Javier Ortiz and Dupont, Yoann and Muller, Benjamin and Romary, Laurent and Sagot, Benoît},
	urldate = {2022-07-21},
	date = {2020-05-11},
	langid = {english},
	file = {Full Text PDF:C\:\\Users\\virgi\\Zotero\\storage\\YHPFNP3J\\Suárez et al. - 2020 - Establishing a New State-of-the-Art for French Nam.pdf:application/pdf},
}

@inproceedings{soudani_adaptation_2018,
	location = {Montpellier, France},
	title = {Adaptation et évaluation de systèmes de reconnaissance et de résolution des entités nommées pour le cas de textes littéraires français du 19ème​ ​ siècle},
	url = {https://hal.archives-ouvertes.fr/hal-01925816},
	booktitle = {Atelier Humanités Numériques Spatialisées ({HumaNS}'2018)},
	author = {Soudani, Aicha and Meherzi, Yosra and Bouhafs, Asma and Frontini, Francesca and Brando, Carmen and Dupont, Yoann and Mélanie-Becquet, Frédérique},
	urldate = {2022-07-21},
	date = {2018-11},
	keywords = {cartographie, cartography, entity recognition, named entity linking, reconnaissance des entités nommées, résolution des entités nommées},
	file = {HAL PDF Full Text:C\:\\Users\\virgi\\Zotero\\storage\\BE6XGQ32\\Soudani et al. - 2018 - Adaptation et évaluation de systèmes de reconnaiss.pdf:application/pdf},
}

@article{koudoro-parfait_reconnaissance_nodate,
	title = {Reconnaissance d'entités nommées sur des sorties {OCR} bruitées: des pistes pour la désambiguïsation morphologique automatique},
	abstract = {Resolution of entity linking issues on noisy {OCR} output : automatic disambiguation tracks.},
	pages = {12},
	author = {Koudoro-Parfait, Caroline and Lejeune, Gaël and Buth, Richy},
	langid = {french},
	file = {Koudoro-Parfait et al. - Reconnaissance d'entités nommées sur des sorties O.pdf:C\:\\Users\\virgi\\Zotero\\storage\\KP9JLWF8\\Koudoro-Parfait et al. - Reconnaissance d'entités nommées sur des sorties O.pdf:application/pdf},
}

@inproceedings{koudoro-parfait_spatial_2021,
	location = {New York, {NY}, {USA}},
	title = {Spatial Named Entity Recognition in Literary Texts: What is the Influence of {OCR} Noise?},
	isbn = {978-1-4503-9102-3},
	url = {https://doi.org/10.1145/3486187.3490206},
	doi = {10.1145/3486187.3490206},
	series = {{GeoHumanities}'21},
	shorttitle = {Spatial Named Entity Recognition in Literary Texts},
	abstract = {Exploring text collections through named entities remains a very common need for scholars. Despite the recent advances of Named Entity Recognition ({NER}) systems, more efficient and easier to use, the task remains problematic when the data is not born digital and thus more prone to Optical Character Recognition errors. In this paper, we investigate the real influence of noise on the extraction of locations in a collection of ten books in French exhibiting different levels of difficulties for {NER} systems (digitization quality, complexity of layout, variation in language). We compare the results of various systems on the "clean" version of the documents and on different {OCRed} versions. We show that {NER} systems do not yield many more errors in noisy documents, most of the errors being already there on a reference version, and in some cases {NER} performs better on noisy versions. According to our results, the main problem is rare entities (especially hapax) which are more likely to disappear from of the output.},
	pages = {13--21},
	booktitle = {Proceedings of the 5th {ACM} {SIGSPATIAL} International Workshop on Geospatial Humanities},
	publisher = {Association for Computing Machinery},
	author = {Koudoro-Parfait, Caroline and Lejeune, Gaël and Roe, Glenn},
	urldate = {2022-07-21},
	date = {2021-11-02},
	keywords = {Digital Humanities, Evaluation, Named Entity Recognition, Natural language Processing, Noise, Optical Character Recognition, Users},
}

@online{noauthor_teklia_nodate,
	title = {Teklia - A Comprehensive Study of Libraries for Named Entity Recognition},
	url = {https://teklia.com/research/publications/monroc2022/},
	urldate = {2022-07-21},
	file = {Teklia - A Comprehensive Study of Libraries for Named Entity Recognition:C\:\\Users\\virgi\\Zotero\\storage\\3YHAFHJB\\monroc2022.html:text/html},
}

@article{rijhwani_zero-shot_2019,
	title = {Zero-Shot Neural Transfer for Cross-Lingual Entity Linking},
	volume = {33},
	rights = {Copyright (c) 2019 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/4670},
	doi = {10.1609/aaai.v33i01.33016924},
	abstract = {Cross-lingual entity linking maps an entity mention in a source language to its corresponding entry in a structured knowledge base that is in a different (target) language. While previous work relies heavily on bilingual lexical resources to bridge the gap between the source and the target languages, these resources are scarce or unavailable for many low-resource languages. To address this problem, we investigate zero-shot cross-lingual entity linking, in which we assume no bilingual lexical resources are available in the source low-resource language. Specifically, we propose pivot-basedentity linking, which leverages information from a highresource “pivot” language to train character-level neural entity linking models that are transferred to the source lowresource language in a zero-shot manner. With experiments on 9 low-resource languages and transfer through a total of54 languages, we show that our proposed pivot-based framework improves entity linking accuracy 17\% (absolute) on average over the baseline systems, for the zero-shot scenario.1 Further, we also investigate the use of language-universal phonological representations which improves average accuracy (absolute) by 36\% when transferring between languages that use different scripts.},
	pages = {6924--6931},
	number = {1},
	journaltitle = {Proceedings of the {AAAI} Conference on Artificial Intelligence},
	author = {Rijhwani, Shruti and Xie, Jiateng and Neubig, Graham and Carbonell, Jaime},
	urldate = {2022-07-23},
	date = {2019-07-17},
	langid = {english},
	note = {Number: 01},
	file = {Full Text PDF:C\:\\Users\\virgi\\Zotero\\storage\\AFAJ37VP\\Rijhwani et al. - 2019 - Zero-Shot Neural Transfer for Cross-Lingual Entity.pdf:application/pdf},
}

@incollection{linhares_pontes_linking_2020,
	location = {New York, {NY}, {USA}},
	title = {Linking Named Entities across Languages using Multilingual Word Embeddings},
	isbn = {978-1-4503-7585-6},
	url = {https://doi.org/10.1145/3383583.3398597},
	abstract = {Digital libraries are online collections of digital objects that can include text, images, audio, or videos in several languages. It has long been observed that named entities ({NEs}) are key to the access to digital library portals as they are contained in most user queries. However, {NEs} can have different spellings for each language which reduces the performance of user queries to retrieve documents across languages. Cross-lingual named entity linking ({XEL}) connects {NEs} from documents in a source language to external knowledge bases in another (target) language. The {XEL} task is especially challenging due to the diversity of {NEs} across languages and contexts. This paper describes an {XEL} system applied and evaluated with several languages pairs including English and various low-resourced languages of different linguistic families such as Croatian, Finnish, Estonian, and Slovenian. We tested this approach to analyze documents and {NEs} in low-resourced languages and link them to the English version of Wikipedia. We present the resulting study of this analysis and the challenges involved in the case of degraded documents from digital libraries. Further works will make an extensive analysis of the impact of our approach on the {XEL} task with {OCRed} documents.},
	pages = {329--332},
	booktitle = {Proceedings of the {ACM}/{IEEE} Joint Conference on Digital Libraries in 2020},
	publisher = {Association for Computing Machinery},
	author = {Linhares Pontes, Elvys and Moreno, Jose G. and Doucet, Antoine},
	urldate = {2022-07-23},
	date = {2020-08-01},
	keywords = {cross-lingual named entity linking, digital library, indexing, multilingual word embeddings},
	file = {Linhares Pontes et al. - 2020 - Linking Named Entities across Languages using Mult.pdf:C\:\\Users\\virgi\\Zotero\\storage\\K379U7SM\\Linhares Pontes et al. - 2020 - Linking Named Entities across Languages using Mult.pdf:application/pdf},
}

@inproceedings{zhou_towards_2019,
	location = {Hong Kong, China},
	title = {Towards Zero-resource Cross-lingual Entity Linking},
	url = {https://aclanthology.org/D19-6127},
	doi = {10.18653/v1/D19-6127},
	abstract = {Cross-lingual entity linking ({XEL}) grounds named entities in a source language to an English Knowledge Base ({KB}), such as Wikipedia. {XEL} is challenging for most languages because of limited availability of requisite resources. However, many works on {XEL} have been on simulated settings that actually use significant resources (e.g. source language Wikipedia, bilingual entity maps, multilingual embeddings) that are not available in truly low-resource languages. In this work, we first examine the effect of these resource assumptions and quantify how much the availability of these resource affects overall quality of existing {XEL} systems. We next propose three improvements to both entity candidate generation and disambiguation that make better use of the limited resources we do have in resource-scarce scenarios. With experiments on four extremely low-resource languages, we show that our model results in gains of 6-20\% end-to-end linking accuracy.},
	pages = {243--252},
	booktitle = {Proceedings of the 2nd Workshop on Deep Learning Approaches for Low-Resource {NLP} ({DeepLo} 2019)},
	publisher = {Association for Computational Linguistics},
	author = {Zhou, Shuyan and Rijhwani, Shruti and Neubig, Graham},
	urldate = {2022-07-23},
	date = {2019-11},
	file = {Full Text PDF:C\:\\Users\\virgi\\Zotero\\storage\\UU8A2UD4\\Zhou et al. - 2019 - Towards Zero-resource Cross-lingual Entity Linking.pdf:application/pdf},
}